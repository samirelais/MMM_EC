{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering pour le Marketing Mix Modeling\n",
    "\n",
    "Ce notebook présente le processus de création des caractéristiques pour le modèle MMM, incluant les transformations d'adstock, la saturation et les caractéristiques temporelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration de Matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Ajouter le répertoire parent au chemin Python\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialiser une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"mmm_feature_engineering\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données prétraitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Charger les données des ventes quotidiennes\n",
    "from src.data.online_retail_loader import OnlineRetailLoader\n",
    "import json\n",
    "\n",
    "# Charger la configuration\n",
    "with open(\"../config/online_retail_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialiser le loader\n",
    "retail_loader = OnlineRetailLoader(spark, \"../config/online_retail_config.json\")\n",
    "\n",
    "# Charger les données retail\n",
    "print(\"Chargement des données retail...\")\n",
    "retail_df = retail_loader.load_retail_data()\n",
    "\n",
    "# Créer les données de ventes quotidiennes\n",
    "print(\"Création des données de ventes quotidiennes...\")\n",
    "daily_sales = retail_loader.create_daily_sales_data(retail_df)\n",
    "\n",
    "# Afficher un aperçu\n",
    "print(\"\\nAperçu des données de ventes quotidiennes:\")\n",
    "daily_sales.show(5)\n",
    "\n",
    "# Créer les données marketing simulées\n",
    "print(\"\\nCréation des données marketing...\")\n",
    "marketing_df = retail_loader.create_marketing_channel_data(retail_df)\n",
    "print(\"\\nAperçu des données marketing:\")\n",
    "marketing_df.show(5)\n",
    "\n",
    "# Créer les facteurs externes\n",
    "print(\"\\nCréation des facteurs externes...\")\n",
    "external_df = retail_loader.create_external_factors(retail_df)\n",
    "print(\"\\nAperçu des facteurs externes:\")\n",
    "external_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering pour les ventes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.features.feature_engineer import FeatureEngineer\n",
    "\n",
    "# Initialiser l'ingénieur de caractéristiques\n",
    "feature_engineer = FeatureEngineer(\"../config/online_retail_config.json\")\n",
    "\n",
    "# Créer des caractéristiques temporelles\n",
    "print(\"Création des caractéristiques temporelles...\")\n",
    "sales_features = feature_engineer.create_seasonality_features(daily_sales)\n",
    "sales_features = feature_engineer.create_holiday_features(sales_features)\n",
    "\n",
    "# Afficher un aperçu des caractéristiques\n",
    "print(\"\\nAperçu des caractéristiques temporelles:\")\n",
    "sales_features.select(\"date\", \"revenue\", \"day_of_week\", \"month\", \"is_weekend\", \"month_sin\", \"month_cos\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering pour les canaux marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Créer des caractéristiques pour les canaux marketing\n",
    "print(\"Création des caractéristiques pour les canaux marketing...\")\n",
    "\n",
    "# Créer des lag features\n",
    "marketing_features = feature_engineer.create_lag_features(\n",
    "    marketing_df, \n",
    "    id_cols=[\"channel\"], \n",
    "    target_cols=[\"spend\"],\n",
    "    lag_periods=[1, 3, 7, 14, 28]\n",
    ")\n",
    "\n",
    "# Créer des rolling features\n",
    "marketing_features = feature_engineer.create_rolling_features(\n",
    "    marketing_features,\n",
    "    id_cols=[\"channel\"],\n",
    "    target_cols=[\"spend\"],\n",
    "    windows=[7, 14, 30]\n",
    ")\n",
    "\n",
    "# Afficher un aperçu\n",
    "print(\"\\nAperçu des caractéristiques marketing:\")\n",
    "marketing_features.select(\"date\", \"channel\", \"spend\", \"spend_lag_7\", \"spend_avg_14d\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modélisation de l'effet Adstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.models.adstock import AdstockModels\n",
    "import pandas as pd\n",
    "\n",
    "# Paramètres d'adstock par canal\n",
    "adstock_params = {\n",
    "    'tv': {'decay_rate': 0.7, 'max_lag': 14, 'saturation_type': 'hill', 'k': 0.7, 'S': 50000},\n",
    "    'radio': {'decay_rate': 0.6, 'max_lag': 7, 'saturation_type': 'hill', 'k': 0.6, 'S': 20000},\n",
    "    'print': {'decay_rate': 0.5, 'max_lag': 21, 'saturation_type': 'hill', 'k': 0.5, 'S': 30000},\n",
    "    'social_media': {'decay_rate': 0.5, 'max_lag': 5, 'saturation_type': 'hill', 'k': 0.8, 'S': 25000},\n",
    "    'search': {'decay_rate': 0.4, 'max_lag': 3, 'saturation_type': 'hill', 'k': 0.9, 'S': 40000},\n",
    "    'email': {'decay_rate': 0.3, 'max_lag': 4, 'saturation_type': 'hill', 'k': 0.7, 'S': 10000},\n",
    "    'display': {'decay_rate': 0.5, 'max_lag': 10, 'saturation_type': 'hill', 'k': 0.6, 'S': 20000}\n",
    "}\n",
    "\n",
    "# Pivoter les données marketing pour avoir une colonne par canal\n",
    "print(\"Pivoter les données marketing...\")\n",
    "pivot_marketing = marketing_df.groupBy(\"date\").pivot(\"channel\").sum(\"spend\").na.fill(0)\n",
    "\n",
    "# Démonstration de l'effet adstock pour un canal\n",
    "channel = 'tv'\n",
    "if channel in pivot_marketing.columns:\n",
    "    print(f\"\\nDémonstration de l'effet adstock pour le canal {channel}:\")\n",
    "    \n",
    "    # Convertir en pandas pour la démonstration\n",
    "    pdf = pivot_marketing.select(\"date\", channel).toPandas()\n",
    "    pdf[\"date\"] = pd.to_datetime(pdf[\"date\"])\n",
    "    pdf = pdf.sort_values(\"date\")\n",
    "    pdf.set_index(\"date\", inplace=True)\n",
    "    \n",
    "    # Appliquer l'adstock géométrique\n",
    "    params = adstock_params[channel]\n",
    "    adstock = AdstockModels.geometric_adstock(\n",
    "        pdf, \n",
    "        channel,\n",
    "        decay_rate=params[\"decay_rate\"],\n",
    "        max_lag=params[\"max_lag\"]\n",
    "    )\n",
    "    \n",
    "    # Appliquer la saturation\n",
    "    transformed = AdstockModels.apply_saturation(\n",
    "        adstock,\n",
    "        saturation_type=params[\"saturation_type\"],\n",
    "        k=params[\"k\"],\n",
    "        S=params[\"S\"]\n",
    "    )\n",
    "    \n",
    "    # Créer un DataFrame pour visualiser\n",
    "    result_pdf = pd.DataFrame({\n",
    "        \"date\": pdf.index,\n",
    "        \"original\": pdf[channel],\n",
    "        \"adstock\": adstock,\n",
    "        \"transformed\": transformed\n",
    "    })\n",
    "    \n",
    "    # Visualiser l'effet\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(result_pdf[\"date\"], result_pdf[\"original\"], label=\"Dépense originale\", color=\"blue\")\n",
    "    plt.plot(result_pdf[\"date\"], result_pdf[\"adstock\"], label=\"Effet adstock\", color=\"red\")\n",
    "    plt.plot(result_pdf[\"date\"], result_pdf[\"transformed\"], label=\"Effet avec saturation\", color=\"green\")\n",
    "    plt.title(f\"Transformation d'adstock et saturation pour {channel}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher la visualisation pour un mois seulement\n",
    "    single_month = result_pdf.iloc[30:60]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(single_month[\"date\"], single_month[\"original\"], label=\"Dépense originale\", color=\"blue\")\n",
    "    plt.plot(single_month[\"date\"], single_month[\"adstock\"], label=\"Effet adstock\", color=\"red\")\n",
    "    plt.plot(single_month[\"date\"], single_month[\"transformed\"], label=\"Effet avec saturation\", color=\"green\")\n",
    "    plt.title(f\"Transformation d'adstock et saturation pour {channel} - Un mois\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Préparation des données finales pour la modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.models.mmm_model import MMMModel\n",
    "\n",
    "# Initialiser le modèle MMM\n",
    "mmm_model = MMMModel(spark, \"../config/online_retail_config.json\")\n",
    "\n",
    "# Prétraiter les données pour la modélisation\n",
    "print(\"Prétraitement des données pour la modélisation...\")\n",
    "preprocessed_df = mmm_model.preprocess_data(daily_sales, marketing_df, external_df)\n",
    "\n",
    "# Afficher un aperçu\n",
    "print(\"\\nAperçu des données prétraitées:\")\n",
    "preprocessed_df.select([col for col in preprocessed_df.columns if col in [\"date\", \"revenue\", \"tv\", \"radio\", \"tv_adstock\", \"is_holiday\"]]).show(5)\n",
    "\n",
    "# Sauvegarder les données prétraitées\n",
    "preprocessed_df.write.mode(\"overwrite\").parquet(\"../data/mmm_features.parquet\")\n",
    "print(\"\\nDonnées prétraitées sauvegardées avec succès!\")\n",
    "\n",
    "# Nombre de lignes et de colonnes\n",
    "rows = preprocessed_df.count()\n",
    "cols = len(preprocessed_df.columns)\n",
    "print(f\"Dimensions finales: {rows} lignes, {cols} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convertir en pandas pour l'analyse des corrélations\n",
    "print(\"Analyse des corrélations...\")\n",
    "pdf = preprocessed_df.toPandas()\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "cols_to_analyze = [\"revenue\"] + \\\n",
    "                  [col for col in pdf.columns if col in config[\"marketing_channels\"]] + \\\n",
    "                  [col for col in pdf.columns if \"_adstock\" in col] + \\\n",
    "                  [\"consumer_confidence\", \"gdp_growth\", \"unemployment\", \"is_holiday\", \"is_promo\"]\n",
    "\n",
    "# Créer une matrice de corrélation\n",
    "corr_matrix = pdf[cols_to_analyze].corr()\n",
    "\n",
    "# Visualiser la matrice de corrélation\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0, fmt=\".2f\")\n",
    "plt.title(\"Matrice de corrélation des caractéristiques principales\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Corrélation avec la revenue\n",
    "revenue_corr = corr_matrix[\"revenue\"].sort_values(ascending=False)\n",
    "print(\"\\nCorrélation des caractéristiques avec la revenue:\")\n",
    "print(revenue_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse de l'importance des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Préparer les données pour l'analyse d'importance\n",
    "print(\"Analyse de l'importance des caractéristiques...\")\n",
    "X = pdf.drop([\"date\", \"revenue\"], axis=1)\n",
    "y = pdf[\"revenue\"]\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer et entraîner un modèle LightGBM\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Créer le dataset LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Extraire l'importance des caractéristiques\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualiser l'importance des caractéristiques\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 des caractéristiques les plus importantes')\n",
    "plt.xlabel('Importance (gain)')\n",
    "plt.ylabel('Caractéristique')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 des caractéristiques les plus importantes:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Préparation pour la modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_end_date = config['data']['train_end_date']\n",
    "test_start_date = config['data']['test_start_date']\n",
    "\n",
    "print(f\"Division en ensembles d'entraînement (jusqu'au {train_end_date}) et de test (à partir du {test_start_date})...\")\n",
    "train_df = preprocessed_df.filter(f\"date <= '{train_end_date}'\")\n",
    "test_df = preprocessed_df.filter(f\"date >= '{test_start_date}'\")\n",
    "\n",
    "# Sauvegarder les ensembles d'entraînement et de test\n",
    "train_df.write.mode(\"overwrite\").parquet(\"../data/train_data.parquet\")\n",
    "test_df.write.mode(\"overwrite\").parquet(\"../data/test_data.parquet\")\n",
    "\n",
    "print(f\"Ensemble d'entraînement: {train_df.count()} lignes\")\n",
    "print(f\"Ensemble de test: {test_df.count()} lignes\")\n",
    "\n",
    "print(\"\\nPréparation des données terminée! Prêt pour la modélisation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Arrêter la session Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
